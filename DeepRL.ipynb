{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf05ffac-e3ef-4fa7-bac9-1f8ba917d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ebf65-f511-411d-92a1-c898fb7112a6",
   "metadata": {},
   "source": [
    "### Q value algorithm\n",
    "+ the basic idea of Q value (value function) is to calculate the value of a given state, with an action at step t using its reward and state value of the next step as the consequence of the action at step t.\n",
    "  + Q($s_t$, $a_t$) = $r_{t+1}$ + gamma * max(Q($s_{t+1}$, $a_{t+1}$)). The right side is called TD target\n",
    "  + The bellman implementation is based on iterative conversion using a learning rate alpha\n",
    "    + $Q_{new}$($s_t$, $a_t$)                                       \n",
    "       = (1-alpha)* $Q_{old}$($s_t$, $a_t$) + alpha * ($r_{t+1} + gamma * max($Q_{old}$($s_{t+1}$, $a{t+1}$)))\n",
    "      \n",
    "+ in deep RL, instead of using the iterative process, we use a deep neural network to learn for a given input state, its output corresponding to each possible action\n",
    "  + the input has the dimension of the state dimension, and output has the dimension of number of actions\n",
    "  + a good example is the LunarLander environment from gym where a state is defined as an eight-dimension vector corresponding to x position, y position, x velocity, y velocity, lander angle, angle velocity, left foot contacts with land (yes/no as 1/0) and right foot contacts with land (yes/no as 1/0)\n",
    "  + for NN network, input_dimension and output_dimensions are 8 and 4 (up, down, left, right) respectively\n",
    "#### Barebone Deep Q-network (DQN)\n",
    "+ selection actions\n",
    "  + select the action with maximum output\n",
    "+ loss function\n",
    "   + instead of iterative process, the loss is directly defined as TD target minus Q-value at (s, a) as below:\n",
    "     + [($r_{t+1}$ + gamma * max(Q($s_{t+1}$, $a_{t+1}$))) - Q($s_t$, $a_t$)]$^2$\n",
    "+  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37b6246-410f-4770-99b8-9235cf38f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Lunar Lander environment\n",
    "import torch.nn as nn\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, dim_inputs, dim_outputs):\n",
    "        super(Network, self).__init__()\n",
    "        # Define a linear transformation layer \n",
    "        self.fc1 = nn.Linear(dim_inputs, 64)\n",
    "        self.fc2 = nn.Linear(64, dim_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.relu(self.fc2(self.fc1(x)))\n",
    "\n",
    "# Instantiate the network\n",
    "network = Network(8, 4)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6d57fb-98df-4255-907b-a266b565cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9176, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def select_action(q_network, state):\n",
    "    actions = q_network(state)\n",
    "    action =  torch.argmax(actions).item()\n",
    "    return action\n",
    "\n",
    "def calculate_loss(q_network, state, action, next_state, reward, done, gamma=0.99):\n",
    "    current_state_q_value = q_network(state)[action]\n",
    "    next_state_q_value = q_network(next_state).max()\n",
    "    target_q_value = reward + gamma * next_state_q_value * (1 - done)\n",
    "    loss = nn.MSELoss()(current_state_q_value, target_q_value)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def describe_episode(episode, episode_reward, step):\n",
    "    print(f\"| Episode:   {episode} | Duration: {step} steps | Return: {episode_reward} |\")\n",
    "\n",
    "# initialize a simple linear NN\n",
    "q_network = Network(8, 4)\n",
    "\n",
    "# simulate a random state and next_state as 8-element vectors\n",
    "state = torch.rand(8)\n",
    "next_state = torch.rand(8)\n",
    "action = select_action(q_network, state)\n",
    "reward = 1\n",
    "gamma = .99\n",
    "done = False\n",
    "\n",
    "calculate_loss(q_network, state, action, next_state, reward, done, gamma)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad82c475-fa34-4e56-911e-7b41bc278a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode:   0 | Duration: 65 steps | Return: -131.4319903801453 |\n",
      "| Episode:   1 | Duration: 61 steps | Return: -125.62647767680724 |\n",
      "| Episode:   2 | Duration: 57 steps | Return: -100.59567031761136 |\n",
      "| Episode:   3 | Duration: 74 steps | Return: -132.08290585974655 |\n",
      "| Episode:   4 | Duration: 55 steps | Return: -94.66884058159405 |\n",
      "| Episode:   5 | Duration: 80 steps | Return: -127.31781506665013 |\n",
      "| Episode:   6 | Duration: 63 steps | Return: -170.77364464951967 |\n",
      "| Episode:   7 | Duration: 69 steps | Return: -175.04003372369544 |\n",
      "| Episode:   8 | Duration: 76 steps | Return: -109.04935455407167 |\n",
      "| Episode:   9 | Duration: 77 steps | Return: -136.6436698244239 |\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    episode_reward = 0    \n",
    "    state = torch.tensor(state)\n",
    "\n",
    "    # Run through steps until done\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = select_action(q_network, state)        \n",
    "        # Take the action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        done = terminated or truncated        \n",
    "        loss = calculate_loss(q_network, state, action, next_state, reward, done)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        # Update the state\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    describe_episode(episode, episode_reward, step)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d5baa-41aa-427b-bc21-8be1d3f5b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacc5d0-36e5-4d44-b771-d72372b6cd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
